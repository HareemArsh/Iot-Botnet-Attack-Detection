{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "g_9v3CAj4yX7",
        "outputId": "875c4302-8617-47c8-c33d-808256317f76"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'benign_traffic.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ce31eb505208>\u001b[0m in \u001b[0;36m<cell line: 149>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;31m# Data preparation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataPrepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;31m# Data preprocessing: training set, validation set, test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_under_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_under_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_under_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_under_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataPreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-ce31eb505208>\u001b[0m in \u001b[0;36mdataPrepare\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdataPrepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"benign_traffic.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'benign_traffic.csv'"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import confusion_matrix, recall_score, classification_report, f1_score\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
        "from sklearn import svm\n",
        "import itertools\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "def Kfold_for_TrainModel(X_train_data, y_train_data):\n",
        "    fold = KFold(len(X_train_data), 5, shuffle=False)\n",
        "    c_params = [0.01, 0.1, 1, 10, 100]\n",
        "    result_tables = pd.DataFrame(columns=['C_parameter', 'Mean recall score'])\n",
        "    result_tables['C_parameter'] = c_params\n",
        "    j = 0\n",
        "    for c_param in c_params:\n",
        "        print('-------------------------------------------')\n",
        "        print('C parameter:', c_param)\n",
        "        print('-------------------------------------------')\n",
        "        print('')\n",
        "        recall_list = []\n",
        "        for iteration, indices in enumerate(fold, start=1):\n",
        "            lr = LogisticRegression(C=c_param, penalty='l1', max_iter=10)\n",
        "            lr.fit(X_train_data.iloc[indices[0], :], y_train_data.iloc[indices[0], :].values.ravel())\n",
        "            y_undersample_pred = lr.predict(X_train_data.iloc[indices[1], :].values)\n",
        "            recall = recall_score(y_train_data.iloc[indices[1], :].values, y_undersample_pred)\n",
        "            f1_score_value = f1_score(y_train_data.iloc[indices[1], :].values, y_undersample_pred)\n",
        "            recall_list.append(recall)\n",
        "            print('Iteration ', iteration, \" Recall: \", recall, \" F1 score: \", f1_score_value)\n",
        "        print('')\n",
        "        print('Average recall: ', np.mean(recall_list))\n",
        "        print('')\n",
        "        result_tables.loc[j, 'Mean recall score'] = np.mean(recall_list)\n",
        "        j = j + 1\n",
        "\n",
        "    result_tables['Mean recall score'] = result_tables['Mean recall score'].astype('float64')\n",
        "    best_c_param = result_tables.loc[result_tables['Mean recall score'].idxmax(), 'C_parameter']\n",
        "    print('*********************************************************************************')\n",
        "    print('Best model corresponds to C parameter = ', best_c_param)\n",
        "    print('*********************************************************************************')\n",
        "    return best_c_param\n",
        "\n",
        "def showData(data):\n",
        "    print(data.shape)\n",
        "    print(data.head())\n",
        "\n",
        "def dataPrepare():\n",
        "    data = pd.read_csv(\"benign_traffic.csv\")\n",
        "    data['Class'] = 0\n",
        "\n",
        "    udp = pd.read_csv(\"miraeudp.csv\")\n",
        "    udp['Class'] = 1\n",
        "\n",
        "    ack = pd.read_csv(\"miraeack.csv\")\n",
        "    ack['Class'] = 1\n",
        "\n",
        "    scan = pd.read_csv(\"miraescan.csv\")\n",
        "    scan['Class'] = 1\n",
        "\n",
        "    syn = pd.read_csv(\"miraesyn.csv\")\n",
        "    syn['Class'] = 1\n",
        "\n",
        "    udpplain = pd.read_csv(\"miraeudpplain.csv\")\n",
        "    udpplain['Class'] = 1\n",
        "\n",
        "    g_combo = pd.read_csv(\"gafgytcombo.csv\")\n",
        "    g_combo['Class'] = 1\n",
        "\n",
        "    g_junk = pd.read_csv(\"gafgytjunk.csv\")\n",
        "    g_junk['Class'] = 1\n",
        "\n",
        "    g_scan = pd.read_csv(\"gafgytscan.csv\")\n",
        "    g_scan['Class'] = 1\n",
        "\n",
        "    g_tcp = pd.read_csv(\"gafgyttcp.csv\")\n",
        "    g_tcp['Class'] = 1\n",
        "\n",
        "    g_udp = pd.read_csv(\"gafgytudp.csv\")\n",
        "    g_udp['Class'] = 1\n",
        "\n",
        "    frames = [data, udp, ack, scan, syn, udpplain, g_combo, g_junk, g_scan, g_tcp, g_udp]\n",
        "    result = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    showData(result)\n",
        "    return result\n",
        "\n",
        "def dataPreprocessing(data):\n",
        "    print('------')\n",
        "    count_class = pd.value_counts(data['Class'], sort=True).sort_index()\n",
        "    print(count_class)\n",
        "    print('------')\n",
        "    X = data.iloc[:, data.columns != 'Class']\n",
        "    y = data.iloc[:, data.columns == 'Class']\n",
        "    positive_sample_count = len(data[data.Class == 1])\n",
        "    print(\"Positive sample count: \", positive_sample_count)\n",
        "    negative_sample_index = np.array(data[data.Class == 0].index)\n",
        "    print(\"Negative sample indices in the dataset (printing the first 5): \", negative_sample_index[:5])\n",
        "    positive_sample_index = data[data.Class == 1].index\n",
        "    random_positive_sample_index = np.random.choice(positive_sample_index, int(1*len(data[data.Class == 0])), replace=False)\n",
        "    print(\"Positive sample indices in the dataset (printing the first 5): \", random_positive_sample_index[:5])\n",
        "    under_sample_index = np.concatenate([random_positive_sample_index, negative_sample_index])\n",
        "    under_sample_data = data.iloc[under_sample_index, :]\n",
        "    X_under_sample = under_sample_data.iloc[:, under_sample_data.columns != 'Class']\n",
        "    y_under_sample = under_sample_data.iloc[:, under_sample_data.columns == 'Class']\n",
        "    print('After under-sampling, the proportion of positive samples in the new dataset: ',\n",
        "          len(under_sample_data[under_sample_data.Class == 1]) / len(under_sample_data))\n",
        "    print('After under-sampling, the proportion of negative samples in the new dataset: ',\n",
        "          len(under_sample_data[under_sample_data.Class == 0]) / len(under_sample_data))\n",
        "    print('After under-sampling, the number of samples in the new dataset: ', len(under_sample_data))\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "    X_train_under_sample, X_test_under_sample, y_train_under_sample, y_test_under_sample = train_test_split(\n",
        "        X_under_sample,\n",
        "        y_under_sample,\n",
        "        test_size=0.3,\n",
        "        random_state=0)\n",
        "    print('Training set sample count: ', len(X_train_under_sample))\n",
        "    print('Testing set sample count: ', len(X_test_under_sample))\n",
        "    return X_train, X_test, y_train, y_test, X_train_under_sample, X_test_under_sample, y_train_under_sample, y_test_under_sample\n",
        "\n",
        "def plot_confusion_matrix(confusion_matrix, classes):\n",
        "    plt.figure()\n",
        "    plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=0)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    thresh = confusion_matrix.max() / 2.\n",
        "    for i, j in itertools.product(range(confusion_matrix.shape[0]), range(confusion_matrix.shape[1])):\n",
        "        plt.text(j, i, confusion_matrix[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if confusion_matrix[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "    print('Precision: ', confusion_matrix[1, 1] / (confusion_matrix[1, 1] + confusion_matrix[0, 1]))\n",
        "    print('Recall: ', confusion_matrix[1, 1] / (confusion_matrix[1, 1] + confusion_matrix[1, 0]))\n",
        "    print('Accuracy: ',\n",
        "          (confusion_matrix[0, 0] + confusion_matrix[1, 1]) / (\n",
        "                      confusion_matrix[0, 0] + confusion_matrix[0, 1] + confusion_matrix[1, 1] + confusion_matrix[1, 0]))\n",
        "    print('*********************************************************************************')\n",
        "\n",
        "# Data preparation\n",
        "result = dataPrepare()\n",
        "# Data preprocessing: training set, validation set, test set\n",
        "X_train, X_test, y_train, y_test, X_train_under_sample, X_test_under_sample, y_train_under_sample, y_test_under_sample = dataPreprocessing(result)\n",
        "\n",
        "# Model\n",
        "# best_c_param = Kfold_for_TrainModel(X_train_under_sample, y_train_under_sample)\n",
        "best_c_param = 10\n",
        "# Evaluation\n",
        "lr = RandomForestClassifier()\n",
        "lr.fit(X_train_under_sample, y_train_under_sample.values.ravel())\n",
        "# Get the test results\n",
        "y_undersample_pred = lr.predict(X_test_under_sample.values)\n",
        "# Build the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test_under_sample, y_undersample_pred)\n",
        "np.set_printoptions(precision=2)\n",
        "class_names = [0, 1]\n",
        "plot_confusion_matrix(conf_matrix, classes=class_names)\n",
        "\n",
        "\n",
        "\n",
        "# Model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train_under_sample, y_train_under_sample.values.ravel())\n",
        "\n",
        "# Evaluation\n",
        "y_undersample_pred_nb = gnb.predict(X_test_under_sample.values)\n",
        "\n",
        "# Build the confusion matrix for Naive Bayes\n",
        "conf_matrix_nb = confusion_matrix(y_test_under_sample, y_undersample_pred_nb)\n",
        "np.set_printoptions(precision=2)\n",
        "class_names = [0, 1]\n",
        "plot_confusion_matrix(conf_matrix_nb, classes=class_names)\n",
        "\n",
        "# Print precision, recall, and accuracy for Naive Bayes\n",
        "print('Precision (Naive Bayes): ', conf_matrix_nb[1, 1] / (conf_matrix_nb[1, 1] + conf_matrix_nb[0, 1]))\n",
        "print('Recall (Naive Bayes): ', conf_matrix_nb[1, 1] / (conf_matrix_nb[1, 1] + conf_matrix_nb[1, 0]))\n",
        "print('Accuracy (Naive Bayes): ',\n",
        "      (conf_matrix_nb[0, 0] + conf_matrix_nb[1, 1]) / (\n",
        "                  conf_matrix_nb[0, 0] + conf_matrix_nb[0, 1] + conf_matrix_nb[1, 1] + conf_matrix_nb[1, 0]))\n",
        "print('*********************************************************************************')\n",
        "\n",
        "\n",
        "# Uncomment the following code to test real data\n",
        "# IsolationForest\n",
        "rng = np.random.RandomState(42)\n",
        "method=''\n",
        "clf = IsolationForest(max_samples=\"auto\", random_state=rng)\n",
        "clf.fit(X_train_under_sample)\n",
        "pred_y = clf.predict(X_train_under_sample)\n",
        "# IsolationForest returns -1 for anomalies and 1 for normal values, so replace 1 with 0, -1 with 1 for anomalies\n",
        "print(pred_y)\n",
        "pred_y = [0 if x==1 else x for x in pred_y]\n",
        "pred_y = [1 if x==-1 else x for x in pred_y]\n",
        "print(pred_y)\n",
        "# Build the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_train_under_sample, pred_y)\n",
        "np.set_printoptions(precision=2)\n",
        "class_names = [0, 1]\n",
        "plot_confusion_matrix(conf_matrix, classes=class_names)\n",
        "# End\n",
        "\n",
        "# One-Class SVM\n",
        "rng = np.random.RandomState(42)\n",
        "method=''\n",
        "clf = svm.OneClassSVM(nu=0.02, kernel=\"rbf\", gamma=0.1)\n",
        "clf.fit(X_train_under_sample)\n",
        "pred_y = clf.predict(X_train_under_sample)\n",
        "# One-Class SVM returns -1 for anomalies and 1 for normal values, so replace 1 with 0, -1 with 1 for anomalies\n",
        "print(pred_y)\n",
        "pred_y = [0 if x==1 else x for x in pred_y]\n",
        "pred_y = [1 if x==-1 else x for x in pred_y]\n",
        "print(pred_y)\n",
        "# Build the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_train_under_sample, pred_y)\n",
        "np.set_printoptions(precision=2)\n",
        "class_names = [0, 1]\n",
        "plot_confusion_matrix(conf_matrix, classes=class_names)\n",
        "# End\n"
      ]
    }
  ]
}